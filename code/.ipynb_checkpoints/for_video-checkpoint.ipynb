{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a2a0b34",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# DSI 37 Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4045a",
   "metadata": {},
   "source": [
    "<a id='part_iv'></a>\n",
    "[Part I](Part_1-Imports#part_i) <br>\n",
    "[Part II](Part-2_Cleaning_and_EDA#part_ii) <br>\n",
    "[Part III](Part_3-EDA_2.ipynb#part_iii) <br>\n",
    "[Part V](Part_5-Implementation.ipynb#part_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416e045",
   "metadata": {},
   "source": [
    "# Part V: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6125f",
   "metadata": {},
   "source": [
    "<a id='part_v'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3829f964",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "[1. Imports (Libraries)](#imports)<br>\n",
    "[2. Sound Files](#mp3)<br>\n",
    "[3. Functions](#fns)<br>\n",
    "[4. Imports (Model)](#unpickle)<br>\n",
    "[5. Testing](#testing)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a31a06d",
   "metadata": {},
   "source": [
    "## 1. Imports (Libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acec9113",
   "metadata": {},
   "source": [
    "<a id='imports'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b389f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dependencies\n",
    "import os\n",
    "from time import process_time\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "100f33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce8a64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import imblearn libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0073d80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, LabelBinarizer\n",
    "\n",
    "from rgf.sklearn import RGFClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c72bed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text to speech \n",
    "from gtts import gTTS\n",
    "from playsound import playsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35738428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video capture\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01130de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48e9e644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ac75f",
   "metadata": {},
   "source": [
    "## 2. Sound Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964a0fd",
   "metadata": {},
   "source": [
    "<a id='mp3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60408bb8",
   "metadata": {},
   "source": [
    "I am going to write a series of recommendations that I think are useful based on the common mistakes people make in the jab and roundhouse kick, and record them as .mp3 files using Google Text To Speech. I will play these files in the final product if the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b80b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of text commands to be converted to audio\n",
    "\n",
    "jab_advice = ['Straighten your left arm!',\n",
    "              'Keep your guard up!',\n",
    "              'Stop leaning forward!',\n",
    "              'Good jab!'\n",
    "             ]\n",
    "\n",
    "kick_advice = ['Turn your hips over!',\n",
    "               'Straighten your right leg!',\n",
    "               'Swing your right arm!',\n",
    "               'Swing your left arm!',\n",
    "               'Kick higher!',\n",
    "               'Good kick!'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f85e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language in which you want to convert\n",
    "language = 'en'\n",
    "  \n",
    "# Passing the text and language to the engine, \n",
    "# here we have marked slow=False. Which tells \n",
    "# the module that the converted audio should \n",
    "# have a high speed\n",
    "\n",
    "for mytext in jab_advice:\n",
    "    myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "    # Saving the converted audio in a mp3 file\n",
    "    myobj.save(f'../data/07-mp3/jab_{mytext.replace(\" \", \"_\")}.mp3')\n",
    "\n",
    "for mytext in kick_advice:\n",
    "    myobj = gTTS(text=mytext, lang=language, slow=False)\n",
    "    # Saving the converted audio in a mp3 file\n",
    "    myobj.save(f'../data/07-mp3/kick_{mytext.replace(\" \", \"_\")}.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5355463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one file\n",
    "playsound('../data/07-mp3/jab_Keep_your_guard_up!.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ebc8d",
   "metadata": {},
   "source": [
    "## 3. Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aadf41",
   "metadata": {},
   "source": [
    "<a id='dictionaries'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63290ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "# See Part C for diagram \n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6854a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684cabf",
   "metadata": {},
   "source": [
    "## 4. Visualisation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08af219",
   "metadata": {},
   "source": [
    "<a id='viz_fns'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7ecb2",
   "metadata": {},
   "source": [
    "These functions draw keypoints and edges on the video based on certain confidence thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92706af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.2):\n",
    "    \"\"\"\n",
    "    Returns high confidence keypoints and edges for visualisation.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "                            the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be visualised.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    keypoints_all = []\n",
    "    keypoint_edges_all = []\n",
    "    edge_colors = []\n",
    "    num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "    for idx in range(num_instances):\n",
    "        kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "        kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "        kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "        kpts_absolute_xy = np.stack(\n",
    "            [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "        kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "            kpts_scores > keypoint_threshold, :]\n",
    "        keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "        if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "            kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "            x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "            y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "            x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "            y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "            line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "            keypoint_edges_all.append(line_seg)\n",
    "            edge_colors.append(color)\n",
    "    if keypoints_all:\n",
    "        keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "    else:\n",
    "        keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "    if keypoint_edges_all:\n",
    "        edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "    else:\n",
    "        edges_xy = np.zeros((0, 2, 2))\n",
    "    return keypoints_xy, edges_xy, edge_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8d7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Draws the keypoint predictions on image.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "            pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "                            the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "                  of the crop region in normalized coordinates (see the init_crop_region\n",
    "                  function below for more detail). If provided, this function will also\n",
    "                  draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "                          Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    height, width, channel = image.shape\n",
    "    aspect_ratio = float(width) / height\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "\n",
    "    # To remove the huge white borders\n",
    "\n",
    "    fig.tight_layout(pad=0)\n",
    "    ax.margins(0)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.axis('off')\n",
    "\n",
    "    im = ax.imshow(image)\n",
    "    line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "    ax.add_collection(line_segments)\n",
    "\n",
    "    # Turn off tick labels\n",
    "\n",
    "    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "    (keypoint_locs, keypoint_edges, edge_colors) = _keypoints_and_edges_for_display(\n",
    "                                                    keypoints_with_scores, height, width)\n",
    "\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "    if keypoint_edges.shape[0]:\n",
    "        line_segments.set_segments(keypoint_edges)\n",
    "        line_segments.set_color(edge_colors)\n",
    "\n",
    "    if keypoint_locs.shape[0]:\n",
    "        scat.set_offsets(keypoint_locs)\n",
    "    \n",
    "    if crop_region is not None:\n",
    "\n",
    "        xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "        ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "        rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "        rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin,ymin),rec_width,rec_height,\n",
    "            linewidth=1,edgecolor='b',facecolor='none')\n",
    "        \n",
    "        ax.add_patch(rect)\n",
    "\n",
    "\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "\n",
    "    image_from_plot = image_from_plot.reshape(\n",
    "        fig.canvas.get_width_height()[::-1] + (3,))\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "    if output_image_height is not None:\n",
    "        output_image_width = int(output_image_height / height * width)\n",
    "        image_from_plot = cv2.resize(\n",
    "            image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "            interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    return image_from_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b30f3",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbc037",
   "metadata": {},
   "source": [
    "<a id='models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ecf5899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    \n",
    "    '''\n",
    "    Input: three sets of (x,y) coordinates (3 tuples)\n",
    "    Output: angle of joint in degrees (1 float)\n",
    "    '''\n",
    "    \n",
    "    a = np.array(a) # First\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle >180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bccaa7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_coords(df, bodypart):\n",
    "    cols = [x for x in df.columns if bodypart in x]\n",
    "    joint_coords = zip(df[cols[1]], df[cols[0]])\n",
    "    return list(joint_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74dbf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_angles(coords_listA, coords_listB, coords_listC):\n",
    "    angles_list = []\n",
    "    for (a, b, c) in zip(coords_listA, coords_listB, coords_listC):\n",
    "        angles_list.append(calculate_angle(a,b,c))\n",
    "    return angles_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de52d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(a, b):\n",
    "    a = np.array(a) \n",
    "    b = np.array(b) \n",
    "    \n",
    "    dist = ((a[0] - b[0])**2 + (a[1] - b[1])**2)**0.5\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18bbd12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_dist(coords_listA, coords_listB):\n",
    "    dist_list = []\n",
    "    for (a, b) in zip(coords_listA, coords_listB):\n",
    "        dist_list.append(distance(a,b))\n",
    "    return dist_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4020211e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features(df):   \n",
    "    \n",
    "    # define (x,y) coordinates for relevant bodyparts\n",
    "    \n",
    "    # left\n",
    "    left_shoulder = joint_coords(df, 'left_shoulder')\n",
    "    left_elbow = joint_coords(df, 'left_elbow')\n",
    "    left_wrist = joint_coords(df, 'left_wrist')\n",
    "    left_hip = joint_coords(df, 'left_hip')\n",
    "    left_knee = joint_coords(df, 'left_knee')\n",
    "    left_ankle = joint_coords(df, 'left_ankle')\n",
    "    left_eye = joint_coords(df, 'left_eye')\n",
    "    \n",
    "    # right\n",
    "    right_shoulder = joint_coords(df, 'right_shoulder')\n",
    "    right_elbow = joint_coords(df, 'right_elbow')\n",
    "    right_wrist = joint_coords(df, 'right_wrist')\n",
    "    right_hip = joint_coords(df, 'right_hip')\n",
    "    right_knee = joint_coords(df, 'right_knee')\n",
    "    right_ankle = joint_coords(df, 'right_ankle')\n",
    "    right_eye = joint_coords(df, 'right_eye')\n",
    "    \n",
    "    # add new columns with the new angles \n",
    "    df['left_elbow_angle'] = all_angles(left_shoulder, left_elbow, left_wrist)\n",
    "    df['left_hip_angle'] = all_angles(left_shoulder, left_hip, left_knee)\n",
    "    df['left_knee_angle'] = all_angles(left_hip, left_knee, left_ankle)\n",
    "\n",
    "    df['right_elbow_angle'] = all_angles(right_shoulder, right_elbow, right_wrist)\n",
    "    df['right_hip_angle'] = all_angles(right_shoulder, right_hip, right_knee)\n",
    "    df['right_knee_angle'] = all_angles(right_hip, right_knee, right_ankle)\n",
    "    \n",
    "    # add new columns with the new distances\n",
    "    df['left_eye_left_wrist'] = all_dist(left_eye, left_wrist)\n",
    "    df['right_eye_right_wrist'] = all_dist(right_eye, right_wrist)\n",
    "    df['left_ankle_right_ankle'] = all_dist(left_ankle, right_ankle)\n",
    "    \n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db4a6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = []\n",
    "for key, value in KEYPOINT_DICT.items():\n",
    "    columns.extend([key + '_y', key + '_x', key + '_conf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b69387",
   "metadata": {},
   "source": [
    "## 6. Imports (Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe11f1",
   "metadata": {},
   "source": [
    "<a id='models'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85b560bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/06-models/rgf_muaythai.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03f92e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[[&#x27;smote&#x27;, SMOTE(random_state=42)],\n",
       "                                       (&#x27;ss&#x27;, StandardScaler()),\n",
       "                                       (&#x27;rgf&#x27;, RGFClassifier())]),\n",
       "             param_grid={&#x27;rgf__l2&#x27;: [0.1], &#x27;rgf__learning_rate&#x27;: [0.8],\n",
       "                         &#x27;rgf__max_leaf&#x27;: [1000], &#x27;rgf__n_iter&#x27;: [None],\n",
       "                         &#x27;rgf__sl2&#x27;: [0.07],\n",
       "                         &#x27;smote__sampling_strategy&#x27;: [&#x27;not minority&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[[&#x27;smote&#x27;, SMOTE(random_state=42)],\n",
       "                                       (&#x27;ss&#x27;, StandardScaler()),\n",
       "                                       (&#x27;rgf&#x27;, RGFClassifier())]),\n",
       "             param_grid={&#x27;rgf__l2&#x27;: [0.1], &#x27;rgf__learning_rate&#x27;: [0.8],\n",
       "                         &#x27;rgf__max_leaf&#x27;: [1000], &#x27;rgf__n_iter&#x27;: [None],\n",
       "                         &#x27;rgf__sl2&#x27;: [0.07],\n",
       "                         &#x27;smote__sampling_strategy&#x27;: [&#x27;not minority&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[[&#x27;smote&#x27;, SMOTE(random_state=42)], (&#x27;ss&#x27;, StandardScaler()),\n",
       "                (&#x27;rgf&#x27;, RGFClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE(random_state=42)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RGFClassifier</label><div class=\"sk-toggleable__content\"><pre>RGFClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[['smote', SMOTE(random_state=42)],\n",
       "                                       ('ss', StandardScaler()),\n",
       "                                       ('rgf', RGFClassifier())]),\n",
       "             param_grid={'rgf__l2': [0.1], 'rgf__learning_rate': [0.8],\n",
       "                         'rgf__max_leaf': [1000], 'rgf__n_iter': [None],\n",
       "                         'rgf__sl2': [0.07],\n",
       "                         'smote__sampling_strategy': ['not minority']})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the pipeline\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ba2887f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Initialized TensorFlow Lite runtime.\n",
      "INFO: Applying 1 TensorFlow Lite delegate(s) lazily.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path = '../streamlit/models/lite-model_movenet_singlepose_thunder_3.tflite')\n",
    "input_size = 256\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2373c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    # TF Lite format expects tensor type of uint8.\n",
    "    input_image = tf.cast(input_image, dtype=tf.float32)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    # Invoke inference.\n",
    "    interpreter.invoke()\n",
    "    # Get the model prediction.\n",
    "    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b81d2bb",
   "metadata": {},
   "source": [
    "## 7. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60ffe3a",
   "metadata": {},
   "source": [
    "<a id='implementation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe0dfc32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guard\n",
      "jab\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "Could not load sound with filename, although URL was good... file:///Users/wynne/Documents/work/GA/projects/P5%20-%20capstone/code/DSI-37-Capstone/code/../data/07-mp3/jab_Straighten_your_left_arm!.mp3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 162\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# jab recommendations\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_elbow_angle \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m175\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mplaysound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/07-mp3/jab_Straighten_your_left_arm!.mp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m left_eye[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m left_ankle[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    164\u001b[0m     playsound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/07-mp3/jab_Stop_leaning_forward!.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/playsound.py:140\u001b[0m, in \u001b[0;36m_playsoundOSX\u001b[0;34m(sound, block)\u001b[0m\n\u001b[1;32m    138\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to load sound, although url was good... \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m sound)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not load sound with filename, although URL was good... \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m sound)\n\u001b[1;32m    141\u001b[0m nssound\u001b[38;5;241m.\u001b[39mplay()\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "\u001b[0;31mPlaysoundException\u001b[0m: Could not load sound with filename, although URL was good... file:///Users/wynne/Documents/work/GA/projects/P5%20-%20capstone/code/DSI-37-Capstone/code/../data/07-mp3/jab_Straighten_your_left_arm!.mp3"
     ]
    }
   ],
   "source": [
    "# Commented out, but use the code below if trying it with a webcam\n",
    "# cap = cv2.VideoCapture(0)    \n",
    "\n",
    "\n",
    "video_path = '../data/01-edited_videos/test_files/01-test.mov'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# setting the stage for the counter/recommender\n",
    "stage = None\n",
    "jab_counter = 0\n",
    "kick_counter = 0\n",
    "\n",
    "# instituting a frame counter to increase model stability\n",
    "# by taking values only every X frames, it decreases the likelihood of the model switching stages wrongly\n",
    "frame_count = 0\n",
    "\n",
    "# extracting the video info from the uploaded video for preparing the output\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_output = cv2.VideoWriter('../data/01-edited_videos/video_with_analysis.mp4', fourcc, frame_fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    # Reshape image\n",
    "    image = frame.copy()\n",
    "    image_height, image_width, channels = image.shape\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "\n",
    "    # Run model inference.\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "    \n",
    "    \n",
    "    # Make sure the camera is placed to the side \n",
    "    # and that the ankles can be seen\n",
    "    # this will prevent the model from making inaccurate predictions it was not trained for\n",
    "    \n",
    "    # first, I will use the angle between the nose and shoulders to determine whether the camera\n",
    "    # is to the side or not\n",
    "    nose = (keypoints_with_scores[0][0][0][1], keypoints_with_scores[0][0][0][0])\n",
    "    left_shoulder = (keypoints_with_scores[0][0][5][1], keypoints_with_scores[0][0][5][0])\n",
    "    right_shoulder = (keypoints_with_scores[0][0][6][1], keypoints_with_scores[0][0][6][0])\n",
    "    \n",
    "    offset_angle = calculate_angle(left_shoulder, nose, right_shoulder)\n",
    "    \n",
    "    # next, to check the left angle confidence\n",
    "    left_ankle_conf = keypoints_with_scores[0][0][15][2]\n",
    "    \n",
    "    # now the command flow\n",
    "    if (offset_angle > 45) and (left_ankle_conf < 0.3):\n",
    "        # Tell user to move camera\n",
    "        \n",
    "        # Big box right in the middle of the screen, 1200 px by 300px\n",
    "        cv2.rectangle(image, (360, 440), (1560, 740), (245, 117, 16), -1) \n",
    "        # top left corner, bottom right corner, colour, line thk (neg = filled)\n",
    "\n",
    "        # Display warning telling them to move the camera\n",
    "        cv2.putText(image, 'Please film yourself from the side'\n",
    "                    , (410,540), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, 'and ensure your ankles are visible'\n",
    "                    , (410,640), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "    \n",
    "    else:\n",
    "        # Extract coordinates\n",
    "        row = keypoints_with_scores[0][0].flatten().tolist()\n",
    "\n",
    "        # Make Dataframe\n",
    "        X = pd.DataFrame([row])\n",
    "        X.columns = columns\n",
    "        X = new_features(X)\n",
    "        \n",
    "        # Make Detections\n",
    "        muay_thai_class = model.predict(X)[0]\n",
    "        muay_thai_prob = model.predict_proba(X)[0]\n",
    "        \n",
    "\n",
    "\n",
    "        # Get status box\n",
    "        cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1) # top left corner, bottom right corner, colour, line thk (neg = filled)\n",
    "\n",
    "        # Display Class\n",
    "        cv2.putText(image, 'CLASS'\n",
    "                    , (145,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, muay_thai_class.split(' ')[0]\n",
    "                    , (140,45), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 4, cv2.LINE_AA)\n",
    "\n",
    "        # Display Probability\n",
    "        cv2.putText(image, 'PROB'\n",
    "                    , (20,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(round(muay_thai_prob[np.argmax(muay_thai_prob)],2))\n",
    "                    , (20,45), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 4, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Jab/Kick counter and recommender logic\n",
    "        \n",
    "        # first to define the relevant parts\n",
    "        left_eye = (keypoints_with_scores[0][0][1][1], keypoints_with_scores[0][0][1][0])\n",
    "        left_wrist = (keypoints_with_scores[0][0][9][1], keypoints_with_scores[0][0][9][0])\n",
    "        left_elbow = (keypoints_with_scores[0][0][7][1], keypoints_with_scores[0][0][7][0])\n",
    "        left_ankle = (keypoints_with_scores[0][0][15][1], keypoints_with_scores[0][0][15][0])\n",
    "        \n",
    "        right_wrist = (keypoints_with_scores[0][0][10][1], keypoints_with_scores[0][0][10][0])\n",
    "        right_elbow = (keypoints_with_scores[0][0][8][1], keypoints_with_scores[0][0][8][0])\n",
    "        right_knee = (keypoints_with_scores[0][0][14][1], keypoints_with_scores[0][0][14][0])\n",
    "        right_hip = (keypoints_with_scores[0][0][12][1], keypoints_with_scores[0][0][12][0])\n",
    "        right_ankle = (keypoints_with_scores[0][0][16][1], keypoints_with_scores[0][0][16][0])\n",
    "        \n",
    "        # next, to define relevant distances and angles\n",
    "        left_eye_left_wrist = distance(left_eye, left_wrist)\n",
    "        left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "        right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "        \n",
    "        # I am redefining the right hip angle to be against the y axis\n",
    "        # this is because using the (right_shoulder, right_hip, right_knee) definition gave bad results\n",
    "        hip_y_axis = (right_hip[0], 0)\n",
    "        right_hip_angle = calculate_angle(hip_y_axis, right_hip, right_knee)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # institute the frame skipping here\n",
    "        if frame_count % 10 == 0:\n",
    "            \n",
    "            # prevent the model from running inferences if probability is low\n",
    "            if muay_thai_prob[np.argmax(muay_thai_prob)] < 0.7:\n",
    "                pass\n",
    "            else:\n",
    "                if muay_thai_class == 'guard':\n",
    "\n",
    "                    # Jab/Kick counter logic\n",
    "                    if stage == None:\n",
    "                        stage = 'guard'\n",
    "                        print(stage)\n",
    "                    elif stage == 'jab':\n",
    "                        stage = 'guard'\n",
    "                        print(stage)\n",
    "                        jab_counter += 1\n",
    "                    elif stage == 'kick':\n",
    "                        stage = 'guard'\n",
    "                        print(stage)\n",
    "                        kick_counter += 1  \n",
    "\n",
    "                    # guard recommendation\n",
    "                    if left_eye_left_wrist > 0.1:\n",
    "                        playsound('../data/07-mp3/jab_Keep_your_guard_up!.mp3')\n",
    "\n",
    "                elif muay_thai_class == 'jab' and stage == 'guard':\n",
    "                    stage = 'jab'\n",
    "                    print(stage)\n",
    "                    \n",
    "                    # jab recommendations\n",
    "                    if left_elbow_angle < 175:\n",
    "                        playsound('../data/07-mp3/jab_Straighten_your_left_arm!.mp3')\n",
    "                    elif left_eye[0] < left_ankle[0]:\n",
    "                        playsound('../data/07-mp3/jab_Stop_leaning_forward!.mp3')\n",
    "                    else:\n",
    "                        playsound('../data/07-mp3/jab_Good_jab!.mp3')\n",
    "                        \n",
    "                elif muay_thai_class == 'kick' and stage == 'guard' and right_hip_angle < 120:\n",
    "                    stage = 'kick'\n",
    "                    print(stage)\n",
    "                    print(right_hip_angle)\n",
    "\n",
    "                    # kick recommendations\n",
    "                    if right_elbow_angle < 110:\n",
    "                        playsound('../data/07-mp3/kick_Swing_your_right_arm!.mp3')\n",
    "                    elif left_wrist[1] > left_eye[1]:\n",
    "                        playsound('../data/07-mp3/kick_Swing_your_left_arm!.mp3')\n",
    "                    elif right_hip_angle > 100:\n",
    "                        playsound('../data/07-mp3/kick_Kick_higher!.mp3')\n",
    "                    elif right_hip[0] > left_hip[0]:\n",
    "                        playsound('../data/07-mp3/kick_Turn_your_hips_over!.mp3')\n",
    "                    else:\n",
    "                        playsound('../data/07-mp3/kick_Good_kick!.mp3')\n",
    "                       \n",
    "                    \n",
    "                        \n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "            \n",
    "    # visualise the classes and probabilities and counts    \n",
    "    \n",
    "    # Get status box\n",
    "    cv2.rectangle(image, (0,940), (250, 1080), (245, 117, 16), -1) # top left corner, bottom right corner, colour, line thk (neg = filled)\n",
    "    \n",
    "    # Jab Reps\n",
    "    cv2.putText(image, 'JABS', (15,980), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, str(jab_counter), \n",
    "                (30,1040), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Kick Reps\n",
    "    cv2.putText(image, 'KICKS', (125,980), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, str(kick_counter), \n",
    "                (130,1040), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Visualise the predictions with image.\n",
    "    display_image = tf.expand_dims(image, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "    display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image(\n",
    "    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "    # advance the frame counter\n",
    "    frame_count += 1\n",
    "    \n",
    "    \n",
    "    # show the overlay \n",
    "    cv2.imshow('Muay Th.AI Trainer', output_overlay)\n",
    "\n",
    "    \n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "video_output.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9699b68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guard\n",
      "jab\n"
     ]
    },
    {
     "ename": "PlaysoundException",
     "evalue": "Could not load sound with filename, although URL was good... file:///Users/wynne/Documents/work/GA/projects/P5%20-%20capstone/code/DSI-37-Capstone/code/../data/07-mp3/jab_Straighten_your_left_arm!.mp3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPlaysoundException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 162\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# jab recommendations\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_elbow_angle \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m175\u001b[39m:\n\u001b[0;32m--> 162\u001b[0m     \u001b[43mplaysound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/07-mp3/jab_Straighten_your_left_arm!.mp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m left_eye[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m left_ankle[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    164\u001b[0m     playsound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/07-mp3/jab_Stop_leaning_forward!.mp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/playsound.py:140\u001b[0m, in \u001b[0;36m_playsoundOSX\u001b[0;34m(sound, block)\u001b[0m\n\u001b[1;32m    138\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to load sound, although url was good... \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m sound)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PlaysoundException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not load sound with filename, although URL was good... \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m sound)\n\u001b[1;32m    141\u001b[0m nssound\u001b[38;5;241m.\u001b[39mplay()\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
      "\u001b[0;31mPlaysoundException\u001b[0m: Could not load sound with filename, although URL was good... file:///Users/wynne/Documents/work/GA/projects/P5%20-%20capstone/code/DSI-37-Capstone/code/../data/07-mp3/jab_Straighten_your_left_arm!.mp3"
     ]
    }
   ],
   "source": [
    "# Commented out, but use the code below if trying it with a webcam\n",
    "# cap = cv2.VideoCapture(0)    \n",
    "\n",
    "\n",
    "video_path = '../data/01-edited_videos/test_files/01-test.mov'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# setting the stage for the counter/recommender\n",
    "stage = None\n",
    "jab_counter = 0\n",
    "kick_counter = 0\n",
    "\n",
    "# instituting a frame counter to increase model stability\n",
    "# by taking values only every X frames, it decreases the likelihood of the model switching stages wrongly\n",
    "frame_count = 0\n",
    "\n",
    "# extracting the video info from the uploaded video for preparing the output\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_output = cv2.VideoWriter('../data/01-edited_videos/video_with_analysis.mp4', fourcc, frame_fps, (width, height))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "\n",
    "    # Reshape image\n",
    "    image = frame.copy()\n",
    "    image_height, image_width, channels = image.shape\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "\n",
    "    # Run model inference.\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "    \n",
    "    \n",
    "    # Make sure the camera is placed to the side \n",
    "    # and that the ankles can be seen\n",
    "    # this will prevent the model from making inaccurate predictions it was not trained for\n",
    "    \n",
    "    # first, I will use the angle between the nose and shoulders to determine whether the camera\n",
    "    # is to the side or not\n",
    "    nose = (keypoints_with_scores[0][0][0][1], keypoints_with_scores[0][0][0][0])\n",
    "    left_shoulder = (keypoints_with_scores[0][0][5][1], keypoints_with_scores[0][0][5][0])\n",
    "    right_shoulder = (keypoints_with_scores[0][0][6][1], keypoints_with_scores[0][0][6][0])\n",
    "    \n",
    "    offset_angle = calculate_angle(left_shoulder, nose, right_shoulder)\n",
    "    \n",
    "    # next, to check the left angle confidence\n",
    "    left_ankle_conf = keypoints_with_scores[0][0][15][2]\n",
    "    \n",
    "    # now the command flow\n",
    "    if (offset_angle > 45) and (left_ankle_conf < 0.3):\n",
    "        # Tell user to move camera\n",
    "        \n",
    "        # Big box right in the middle of the screen, 1200 px by 300px\n",
    "        cv2.rectangle(image, (360, 440), (1560, 740), (245, 117, 16), -1) \n",
    "        # top left corner, bottom right corner, colour, line thk (neg = filled)\n",
    "\n",
    "        # Display warning telling them to move the camera\n",
    "        cv2.putText(image, 'Please film yourself from the side'\n",
    "                    , (410,540), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "        cv2.putText(image, 'and ensure your ankles are visible'\n",
    "                    , (410,640), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 0), 4, cv2.LINE_AA)\n",
    "    \n",
    "    else:\n",
    "        # Extract coordinates\n",
    "        row = keypoints_with_scores[0][0].flatten().tolist()\n",
    "\n",
    "        # Make Dataframe\n",
    "        X = pd.DataFrame([row])\n",
    "        X.columns = columns\n",
    "        X = new_features(X)\n",
    "        \n",
    "        # Make Detections\n",
    "        muay_thai_class = model.predict(X)[0]\n",
    "        muay_thai_prob = model.predict_proba(X)[0]\n",
    "        \n",
    "\n",
    "\n",
    "        # Get status box\n",
    "        cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1) # top left corner, bottom right corner, colour, line thk (neg = filled)\n",
    "\n",
    "        # Display Class\n",
    "        cv2.putText(image, 'CLASS'\n",
    "                    , (145,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, muay_thai_class.split(' ')[0]\n",
    "                    , (140,45), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 4, cv2.LINE_AA)\n",
    "\n",
    "        # Display Probability\n",
    "        cv2.putText(image, 'PROB'\n",
    "                    , (20,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, str(round(muay_thai_prob[np.argmax(muay_thai_prob)],2))\n",
    "                    , (20,45), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 4, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Jab/Kick counter and recommender logic\n",
    "        \n",
    "        # first to define the relevant parts\n",
    "        left_eye = (keypoints_with_scores[0][0][1][1], keypoints_with_scores[0][0][1][0])\n",
    "        left_wrist = (keypoints_with_scores[0][0][9][1], keypoints_with_scores[0][0][9][0])\n",
    "        left_elbow = (keypoints_with_scores[0][0][7][1], keypoints_with_scores[0][0][7][0])\n",
    "        left_ankle = (keypoints_with_scores[0][0][15][1], keypoints_with_scores[0][0][15][0])\n",
    "        \n",
    "        right_wrist = (keypoints_with_scores[0][0][10][1], keypoints_with_scores[0][0][10][0])\n",
    "        right_elbow = (keypoints_with_scores[0][0][8][1], keypoints_with_scores[0][0][8][0])\n",
    "        right_knee = (keypoints_with_scores[0][0][14][1], keypoints_with_scores[0][0][14][0])\n",
    "        right_hip = (keypoints_with_scores[0][0][12][1], keypoints_with_scores[0][0][12][0])\n",
    "        right_ankle = (keypoints_with_scores[0][0][16][1], keypoints_with_scores[0][0][16][0])\n",
    "        \n",
    "        # next, to define relevant distances and angles\n",
    "        left_eye_left_wrist = distance(left_eye, left_wrist)\n",
    "        left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)\n",
    "        right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)\n",
    "        \n",
    "        # I am redefining the right hip angle to be against the y axis\n",
    "        # this is because using the (right_shoulder, right_hip, right_knee) definition gave bad results\n",
    "        hip_y_axis = (right_hip[0], 0)\n",
    "        right_hip_angle = calculate_angle(hip_y_axis, right_hip, right_knee)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # institute the frame skipping here\n",
    "        if frame_count % 10 == 0:\n",
    "            \n",
    "            # prevent the model from running inferences if probability is low\n",
    "            if muay_thai_prob[np.argmax(muay_thai_prob)] < 0.7:\n",
    "                pass\n",
    "            else:\n",
    "                if muay_thai_class == 'guard':\n",
    "\n",
    "                    # Jab/Kick counter logic\n",
    "                    if stage == None:\n",
    "                        stage = 'guard'\n",
    "                        print(stage)\n",
    "                    elif stage == 'jab':\n",
    "                        stage = 'guard'\n",
    "                        print(stage)\n",
    "                        jab_counter += 1\n",
    "                    elif stage == 'kick':\n",
    "                        stage = 'guard'\n",
    "                        print(stage)\n",
    "                        kick_counter += 1  \n",
    "\n",
    "                    # guard recommendation\n",
    "                    if left_eye_left_wrist > 0.1:\n",
    "                        playsound('../data/07-mp3/jab_Keep_your_guard_up!.mp3')\n",
    "\n",
    "                elif muay_thai_class == 'jab' and stage == 'guard':\n",
    "                    stage = 'jab'\n",
    "                    print(stage)\n",
    "                    \n",
    "                    # jab recommendations\n",
    "                    if left_elbow_angle < 175:\n",
    "                        playsound('../data/07-mp3/jab_Straighten_your_left_arm!.mp3')\n",
    "                    elif left_eye[0] < left_ankle[0]:\n",
    "                        playsound('../data/07-mp3/jab_Stop_leaning_forward!.mp3')\n",
    "                    else:\n",
    "                        playsound('../data/07-mp3/jab_Good_jab!.mp3')\n",
    "                        \n",
    "                elif muay_thai_class == 'kick' and stage == 'guard' and right_hip_angle < 120:\n",
    "                    stage = 'kick'\n",
    "                    print(stage)\n",
    "                    print(right_hip_angle)\n",
    "\n",
    "                    # kick recommendations\n",
    "                    if right_elbow_angle < 110:\n",
    "                        playsound('../data/07-mp3/kick_Swing_your_right_arm!.mp3')\n",
    "                    elif left_wrist[1] > left_eye[1]:\n",
    "                        playsound('../data/07-mp3/kick_Swing_your_left_arm!.mp3')\n",
    "                    elif right_hip_angle > 100:\n",
    "                        playsound('../data/07-mp3/kick_Kick_higher!.mp3')\n",
    "                    elif right_hip[0] > left_hip[0]:\n",
    "                        playsound('../data/07-mp3/kick_Turn_your_hips_over!.mp3')\n",
    "                    else:\n",
    "                        playsound('../data/07-mp3/kick_Good_kick!.mp3')\n",
    "                       \n",
    "                    \n",
    "                        \n",
    "        else:\n",
    "            pass\n",
    "                \n",
    "            \n",
    "    # visualise the classes and probabilities and counts    \n",
    "    \n",
    "    # Get status box\n",
    "    cv2.rectangle(image, (0,940), (250, 1080), (245, 117, 16), -1) # top left corner, bottom right corner, colour, line thk (neg = filled)\n",
    "    \n",
    "    # Jab Reps\n",
    "    cv2.putText(image, 'JABS', (15,980), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, str(jab_counter), \n",
    "                (30,1040), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Kick Reps\n",
    "    cv2.putText(image, 'KICKS', (125,980), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,0), 1, cv2.LINE_AA)\n",
    "    cv2.putText(image, str(kick_counter), \n",
    "                (130,1040), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 2, (255,255,255), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Visualise the predictions with image.\n",
    "    display_image = tf.expand_dims(image, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "    display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image(\n",
    "    np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "    # advance the frame counter\n",
    "    frame_count += 1\n",
    "    \n",
    "    \n",
    "    # show the overlay \n",
    "    cv2.imshow('Muay Th.AI Trainer', output_overlay)\n",
    "\n",
    "    \n",
    "\n",
    "    if cv2.waitKey(10) & 0xFF==ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "video_output.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c3df62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
